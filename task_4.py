# -*- coding: utf-8 -*-
"""Task-4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g3N-GNPFoPfK-HsUaW9iGdyevKLYxKuK

Task-4
 MACHINE LEARNING MODEL IMPLEMENTATION
"""

# Imports
import numpy as np
import pandas as pd
import nltk
from nltk.corpus import stopwords
import string
import matplotlib.pyplot as plt
import seaborn as sns

# Download Stopwords Package
nltk.download('stopwords')

from google.colab import files
uploaded=files.upload()

# Load Data

df = pd.read_csv("spam_ham_dataset.csv")

# Clean Data
df = df.rename(columns={"label_num":"spam"})
df = df[["spam","text"]]
df

print("--------DF info--------")
# Print df Info
df.info()
print("-----------------------")

# Print OG Shape
og_rows = df.shape[0]
print("\nOriginal Shape:",df.shape)

# Drop Duplicates
df.drop_duplicates(inplace=True)

# Print Cleaned Shape
cleaned_rows = df.shape[0]
print("Cleaned Shape:",df.shape)

# Print number of rows dropped
difference = og_rows - cleaned_rows
print(f"\nDropped {difference} duplicated rows\n")

# Show amount of missing data for each column
print("Missing Data:",end="")
df.isnull().sum()

# Function to process text
def process_text(text):
    '''
    1. Remove punctuation
    2. Remove stopwords
    3. Return list of cleaned text words
    '''

    # 1. Remove punctuation
    nopunc = [char for char in text if char not in string.punctuation]
    nopunc = ''.join(nopunc)

    # 2 Remove stopwords
    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]

    # 3. Return list of cleaned text words
    return clean_words

# Show the tokenization (a list of tokens also called lemmas)
df['text'].head().apply(process_text)

# Example
message1 = "hello world hello hello world play"
message2 = "test test test one test hello"
print(message1, end="\n\n")

# Convert the text to a matrix of token counts
from sklearn.feature_extraction.text import CountVectorizer
bow4 = CountVectorizer(analyzer=process_text).fit_transform([[message1], [message2]])
print(bow4, end="\n\n")
print("bow4 shape:",bow4.shape)

# Convert a collection of text to a matrix of tokens
from sklearn.feature_extraction.text import CountVectorizer
messages_bow = CountVectorizer(analyzer = process_text).fit_transform(df['text'])

# Split data into 80% training / 20% testing
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(messages_bow, df['spam'], test_size=0.20, random_state = 0)

# Display shape of messages_bow
print("messages_bow shape:", messages_bow.shape, end="")

# Create and Train Naive Bayes Classifier
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB().fit(X_train, y_train)

# Training Data Model Evaluation

# Print Predictions on Train
print("Predictions:",classifier.predict(X_train))

# Print Actual Values
print("\nY Train Values:",y_train.values)

# Evaluate model on the training data set
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
pred = classifier.predict(X_train)
print('\033[1m'+ "\n\t\tClassifcation Report (y-train ~ pred)" +'\033[0m')
print(classification_report(y_train, pred))

# Display Accuracy on Training Set
print("Accuracy:", accuracy_score(y_train, pred))

# Display Confusion Matrix for Training Set Predictions
print('\nConfusion Matrix:\n', confusion_matrix(y_train, pred))
sns.heatmap(confusion_matrix(y_train, pred), annot=True, fmt='g', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.gcf().set_size_inches(7,6)
plt.title("Confucian Matrix")
plt.show()

# Testing Data Model Evaluation

# Print Predictions on Test
print("Predictions:",classifier.predict(X_test))

# Print Actual Values
print("\nY Test Values:",y_test.values)

# Evaluate model on the training data set
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
pred = classifier.predict(X_test)
print('\033[1m'+ "\n\t\tClassifcation Report (y-test ~ pred)" +'\033[0m')
print(classification_report(y_test, pred))

# Display Accuracy for testing data set
print("Accuracy:", accuracy_score(y_test, pred))

# Display Confusion Matrix for testing data set
print('\nConfusion Matrix:\n', confusion_matrix(y_test, pred))
sns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='g', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.gcf().set_size_inches(7,6)
plt.title("Confucian Matrix")
plt.show()

